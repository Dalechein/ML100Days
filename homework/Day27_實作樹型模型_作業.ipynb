{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作業目的: 實作樹型模型\n",
    "\n",
    "在本次課程中實作了以Entropy計算訊息增益的決策樹模型，而計算訊息增益的方法除了Entropy只外還有Gini。因此本次作業希望讀者實作以Gini計算\n",
    "\n",
    "訊息增益，且基於課程的決策樹模型建構隨機森林模型。\n",
    "\n",
    "在作業資料夾中的`decision_tree_functions.py`檔案有在作業中實作的所有函式，在實作作業中可以充分利用已經寫好的函式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: 使用Gini計算訊息增益\n",
    "\n",
    "$$\n",
    "Gini = \\sum_{i=1}^cp(i)(1-p(i)) = 1 - \\sum_{i=1}^cp(i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>diameter</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Green</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Red</td>\n",
       "      <td>3.2</td>\n",
       "      <td>Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Red</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Grape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Red</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Grape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yellow</td>\n",
       "      <td>3.3</td>\n",
       "      <td>Lemon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    color  diameter  label\n",
       "0   Green       3.1  Apple\n",
       "1     Red       3.2  Apple\n",
       "2     Red       1.2  Grape\n",
       "3     Red       1.0  Grape\n",
       "4  Yellow       3.3  Lemon"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用與課程中相同的假資料\n",
    "training_data = [\n",
    "    ['Green', 3.1, 'Apple'],\n",
    "    ['Red', 3.2, 'Apple'],\n",
    "    ['Red', 1.2, 'Grape'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Yellow', 3.3, 'Lemon'],\n",
    "    ['Yellow', 3.1, 'Lemon'],\n",
    "    ['Green', 3, 'Apple'],\n",
    "    ['Red', 1.1, 'Grape'],\n",
    "    ['Yellow', 3, 'Lemon'],\n",
    "    ['Red', 1.2, 'Grape'],\n",
    "]\n",
    "\n",
    "header = [\"color\", \"diameter\", \"label\"]\n",
    "\n",
    "df = pd.DataFrame(data=training_data, columns=header)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割測試集與訓練集  \n",
    "def train_test_split_t(df, test_size=0.1):\n",
    "    \n",
    "    if isinstance(test_size, float):\n",
    "        test_size = round(test_size * len(df))\n",
    "\n",
    "    #以隨機的方式取的測試集資料點的index\n",
    "    indices = list(df.index)\n",
    "    test_indices = random.sample(population=indices, k=test_size)\n",
    "\n",
    "    #分割測試集與訓練集\n",
    "    test_df = df.loc[test_indices]\n",
    "    train_df = df.drop(test_indices)\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查資料是否都為同一類別\n",
    "def check_purity(data):\n",
    "    '''Function to check if input data all belong to the same class\n",
    "    Parameter\n",
    "    ---------\n",
    "    data: list\n",
    "        Input data\n",
    "    '''\n",
    "    #取的資料的label訊息\n",
    "    labels = data[:, -1]\n",
    "    \n",
    "    #檢查是否所有的label都為同一種\n",
    "    unique_classes = np.unique(labels)\n",
    "    \n",
    "    if len(unique_classes) == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根據給定的資料，取得每個特徵(feature)可能做為樹型模型分割節點的值\n",
    "# 可能作為分割節點得值即為每個特徵的獨特值(unique value)\n",
    "def get_potential_splits(data, random_features=None):\n",
    "    '''Function to get all potential split value for tree base model\n",
    "    Parameter\n",
    "    ---------\n",
    "    data: list\n",
    "        Input data\n",
    "    '''\n",
    "    \n",
    "    potential_splits = {}\n",
    "    _, n_columns = data.shape\n",
    "    column_indices = list(range(n_columns - 1)) #此處的-1是為了扣掉label的欄位\n",
    "    \n",
    "    if random_features and random_features <= len(column_indices):\n",
    "        #隨機選取特徵進行訓練\n",
    "        column_indices = random.sample(population=column_indices, k=random_features)\n",
    "    \n",
    "    for column_index in column_indices:    \n",
    "        \n",
    "        #根據欄位取的特徵的獨特值(unique values)\n",
    "        values = data[:, column_index]\n",
    "        unique_values = np.unique(values)\n",
    "        \n",
    "        #將取得的可能分割值除存在potential_split的字典中(key=特徵欄位的index, value:此特徵可能的分割值)\n",
    "        potential_splits[column_index] = unique_values\n",
    "    \n",
    "    return potential_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#由給定的輸入DataFrame給個特徵值的型態(數值型特徵或類別型特徵)\n",
    "def determine_type_of_feature(df):\n",
    "    '''Function to get features types\n",
    "    Parameter\n",
    "    ---------\n",
    "    df: pd.DataFrame\n",
    "        Input raw pd.DataFrame data\n",
    "    '''\n",
    "    \n",
    "    feature_types = []\n",
    "    \n",
    "    #若特徵的獨特值個數較少，及當作類別型特徵資料(若為數值型，獨特值個數應該會很多)\n",
    "    #此處簡易的將判斷方法設為資料個數的1/3次方，此值可以自行修改選較為適合的個數\n",
    "    n_unique_values_treshold = int(len(df)**(1/3))\n",
    "    \n",
    "    for feature in df.columns:\n",
    "        if feature != \"label\":\n",
    "            unique_values = df[feature].unique()\n",
    "            rep_value = unique_values[0] #選出一個值做此特徵的代表\n",
    "\n",
    "            if (isinstance(rep_value, str)) or (len(unique_values) <= n_unique_values_treshold):\n",
    "                feature_types.append(\"categorical\")\n",
    "            else:\n",
    "                feature_types.append(\"continuous\")\n",
    "    \n",
    "    return feature_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根據給定的資料、欲採用特徵欄位指標(index)與欲採用的分割值，來取的分割節點分割後的左節點資料與右節點資料\n",
    "def split_data(data, split_column, split_value):\n",
    "    '''Function to splitted left and right nodes\n",
    "    Parameter\n",
    "    ---------\n",
    "    data: list\n",
    "        Input data\n",
    "    split_column: int\n",
    "        index for feature column\n",
    "    split_value: float or int or string\n",
    "        value to be used as split benchmark\n",
    "    '''\n",
    "    \n",
    "    #取得用來分割的特徵欄位\n",
    "    split_column_values = data[:, split_column]\n",
    "\n",
    "    #依據欄位值的型態(數值型特徵或類別型特徵)來進行節點分割\n",
    "    type_of_feature = FEATURE_TYPES[split_column]\n",
    "    \n",
    "    if type_of_feature == \"continuous\":\n",
    "        #數值型特徵分割\n",
    "        data_left = data[split_column_values <= split_value]\n",
    "        data_right = data[split_column_values >  split_value]\n",
    "    else:\n",
    "        #類別型特徵分割\n",
    "        data_left = data[split_column_values == split_value]\n",
    "        data_right = data[split_column_values != split_value]\n",
    "    \n",
    "    return data_left, data_right\n",
    "\n",
    "\n",
    "# 根據給定的資料與任務類型(回歸或分類)來產生終端節點\n",
    "def create_leaf(data, task_type):\n",
    "    '''Function to create leaf node\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: list\n",
    "        Input data\n",
    "    task_type: str\n",
    "        indicate the type of tree (regression or classification)\n",
    "    '''\n",
    "    \n",
    "    #取資料的label欄位\n",
    "    label_column = data[:, -1]\n",
    "\n",
    "    #節點無法再分割時，此節點即成為終端節點(Leaf node)，在終端節點需要取得此節點最後分類的類別(在此節點的所有樣本皆被分類為同一個類別)\n",
    "    if task_type == \"regression\":\n",
    "        #回歸任務\n",
    "        leaf = np.mean(label_column)\n",
    "    else:\n",
    "        #分類任務\n",
    "        #取得所有輸入資料的獨立類別與其個數\n",
    "        unique_classes, counts_unique_classes = np.unique(label_column, return_counts=True)\n",
    "        #print(f'unique_classes, counts_unique_classes: {unique_classes, counts_unique_classes}')\n",
    "        \n",
    "        #以個數最多的類別，作為此節點的輸出類別\n",
    "        index = counts_unique_classes.argmax()\n",
    "        leaf = unique_classes[index]\n",
    "        #print(f'leaf: {leaf}')\n",
    "    \n",
    "    return leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gini impurity\n",
    "def calculate_gini(data):\n",
    "    \n",
    "    #取的資料的label訊息\n",
    "    labels = data[:,-1]\n",
    "        \n",
    "    #取得所有輸入資料的獨立類別與其個數\n",
    "    unique_classes, count_unique_classes = np.unique(labels, return_counts=True)\n",
    "    \n",
    "    #計算機率\n",
    "    probabilities = count_unique_classes/count_unique_classes.sum()\n",
    "    \n",
    "    gini = 1\n",
    "    \n",
    "    #計算gini impurity\n",
    "    for x in probabilities:\n",
    "        gini -= x ** 2\n",
    "    \n",
    "    return gini\n",
    "\n",
    "#取得左節點與右節點訊息合\n",
    "#計算總訊息增益\n",
    "def calculate_overall_metric(data_below, data_above, metric_function):\n",
    "    #n = len(data_below) + len(data_above): (8, 2, 6)\n",
    "    n = len(data_below) + len(data_above)\n",
    "\n",
    "    p_data_below = len(data_below) / n\n",
    "    p_data_above = len(data_above) / n\n",
    "\n",
    "    overall_metric =  (p_data_below * metric_function(data_below) \n",
    "                     + p_data_above * metric_function(data_above))\n",
    "    \n",
    "    return overall_metric\n",
    "\n",
    "\n",
    "#以迴圈的方式計算所有可能分割值的訊息增益，取的最佳的分割特徵與值(訊息增益最大)\n",
    "def determine_best_split(data, potential_splits, metric_function, task_type='classification'):\n",
    "    \n",
    "    #紀錄是否為樹的第一層(第一次回圈)\n",
    "    first_iteration = True\n",
    "    \n",
    "    for column_index in potential_splits:\n",
    "        for value in potential_splits[column_index]:\n",
    "            \n",
    "            #根據給定的特徵與分割值分割資料為左節點、右節點\n",
    "            data_left, data_right = split_data(data, split_column=column_index, split_value=value)\n",
    "            \n",
    "            #判斷是回歸樹亦或分類樹\n",
    "            if task_type == \"regression\":\n",
    "                #回歸樹\n",
    "                current_overall_metric = calculate_overall_metric(data_left, data_right, metric_function=metric_function)\n",
    "            else:\n",
    "                #分類樹\n",
    "                current_overall_metric = calculate_overall_metric(data_left, data_right, metric_function=metric_function)\n",
    "\n",
    "            if first_iteration or current_overall_metric <= best_overall_metric:\n",
    "                first_iteration = False\n",
    "                \n",
    "                best_overall_metric = current_overall_metric\n",
    "                best_split_column = column_index\n",
    "                best_split_value = value\n",
    "    \n",
    "    return best_split_column, best_split_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decision_tree():\n",
    "    '''Decision Tree model\n",
    "    Parameters\n",
    "    -----------\n",
    "    metric_function: function\n",
    "        the metric function used to calculate information gain\n",
    "    task_type: str\n",
    "        indicate the type of tree (regression or classification)\n",
    "    counter: int\n",
    "        counter for recording number of splits\n",
    "    min_samples: int\n",
    "        minimum number of samples for a node to be able to split\n",
    "    max_depth: int\n",
    "        Maximum depth for the decision tree\n",
    "    '''\n",
    "    def __init__(self, metric_function, task_type='classification', counter=0, min_samples=2, max_depth=5):\n",
    "        \n",
    "        self.metric_function = metric_function\n",
    "        self.task_type = task_type\n",
    "        self.counter = counter\n",
    "        self.min_samples = min_samples\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def fit(self, df):\n",
    "        '''\n",
    "        df: pd.DataFrame\n",
    "            input raw DataFrame data\n",
    "        '''\n",
    "        # 資料準備\n",
    "        if self.counter == 0:\n",
    "            #若為第一次分割，取出資料特徵的欄位與其對應的型態\n",
    "            global COLUMN_HEADERS, FEATURE_TYPES\n",
    "\n",
    "            #取得資料特徵欄位\n",
    "            COLUMN_HEADERS = df.columns\n",
    "            #取的特徵型態\n",
    "            FEATURE_TYPES = determine_type_of_feature(df)\n",
    "            #取得資料特徵值\n",
    "            data = df.values\n",
    "        else:\n",
    "            #取得資料特徵值\n",
    "            data = df           \n",
    "\n",
    "        # 終端節點處理(leaf)\n",
    "        # 若資料都屬於同一種類別、資料個數小於最小可分割個數、樹的深度大於最大深度，節點即屬於終端節點(leaf)\n",
    "        if (check_purity(data)) or (len(data) < self.min_samples) or (self.counter == self.max_depth):\n",
    "            leaf = create_leaf(data, self.task_type)\n",
    "            return leaf\n",
    "\n",
    "        # 分割節點\n",
    "        else:    \n",
    "            self.counter += 1\n",
    "\n",
    "            # 節點分割的左節點與右節點\n",
    "            potential_splits = get_potential_splits(data)\n",
    "            split_column, split_value = determine_best_split(data, potential_splits,\n",
    "                                                             self.metric_function, self.task_type)\n",
    "            data_left, data_right = split_data(data, split_column, split_value)\n",
    "\n",
    "            # 若分割後的左節點或右節點sample個數為零(代表母節點即無法在分割)\n",
    "            if len(data_left) == 0 or len(data_right) == 0:\n",
    "                # 取出此節點\n",
    "                leaf = create_leaf(data, self.task_type)\n",
    "                return leaf\n",
    "\n",
    "            # 取得分割節點的分割依據(特徵與分切值)\n",
    "            feature_name = COLUMN_HEADERS[split_column]\n",
    "            type_of_feature = FEATURE_TYPES[split_column]\n",
    "\n",
    "            if type_of_feature == \"continuous\":\n",
    "                #連續型數值\n",
    "                question = \"{} <= {}\".format(feature_name, split_value)\n",
    "            else:\n",
    "                #類別型數值\n",
    "                question = \"{} = {}\".format(feature_name, split_value)\n",
    "\n",
    "                \n",
    "            # 建構子樹(sub-tree)\n",
    "            sub_tree = {question: []}\n",
    "\n",
    "            # 已遞迴的方式取建構完整決策樹    \n",
    "            yes_answer = self.fit(data_left)\n",
    "            no_answer = self.fit(data_right)\n",
    "            #yes_answer = decision_tree(data_left, metric_function, task_type, counter, min_samples, max_depth)\n",
    "            #no_answer = decision_tree(data_right, metric_function, task_type, counter, min_samples, max_depth)\n",
    "\n",
    "            #若左節點與右節點分割的結果相同，則此節點及不需再進行分割\n",
    "            #此情形會發生在此節點資料個數小於min_samples或樹深度大於max_depth\n",
    "            if yes_answer == no_answer:\n",
    "                sub_tree = yes_answer\n",
    "            else:\n",
    "                sub_tree[question].append(yes_answer)\n",
    "                sub_tree[question].append(no_answer)\n",
    "            \n",
    "            self.sub_tree = sub_tree\n",
    "            \n",
    "            return sub_tree\n",
    "        \n",
    "    def pred(self, example, tree):\n",
    "        # 使用訓練好的決策樹進行預測\n",
    "        \n",
    "        #取得分割節點(由上到下)\n",
    "        question = list(tree.keys())[0]\n",
    "        feature_name, comparison_operator, value = question.split(\" \")\n",
    "\n",
    "        #以節點分割問題分類資料\n",
    "        if comparison_operator == \"<=\":\n",
    "            #數值型資料\n",
    "            if example[feature_name] <= float(value):\n",
    "                answer = tree[question][0]\n",
    "            else:\n",
    "                answer = tree[question][1]\n",
    "        else:\n",
    "            #類別型資料\n",
    "            if str(example[feature_name]) == value:\n",
    "                answer = tree[question][0]\n",
    "            else:\n",
    "                answer = tree[question][1]\n",
    "        \n",
    "        # 若分類完成，返回分類結果\n",
    "        if not isinstance(answer, dict):\n",
    "            return answer\n",
    "        else:\n",
    "            #繼續往下分類\n",
    "            residual_tree = answer\n",
    "            return self.pred(example, residual_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'diameter <= 1.2': ['Grape', {'color = Yellow': ['Lemon', 'Apple']}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#分割資料集\n",
    "train_df, test_df = train_test_split_t(df, 0.2)\n",
    "\n",
    "#clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "#以Gini inpurity作為metric_function訓練決策樹\n",
    "tree = decision_tree(calculate_gini, 'classification', 0, min_samples=2, max_depth=5)\n",
    "tree.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lemon'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 以建構好的樹進行預測\n",
    "sample = test_df.iloc[0]\n",
    "tree.pred(sample, tree.sub_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "color       Yellow\n",
       "diameter         3\n",
       "label        Lemon\n",
       "Name: 8, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: 實作隨機森林\n",
    "利用決策樹來實作隨機森林模型，讀者可參考隨機森林課程講義。\n",
    "\n",
    "此份作業只要求讀者實作隨機sample訓練資料，而隨機sample特徵進行訓練的部分，讀者可以參考`decision_tree_functions.py`中的`get_potential_splits`與`decision_tree`部分(新增參數`random_features`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    ['Green', 3.1, 'Apple'],\n",
    "    ['Red', 3.2, 'Apple'],\n",
    "    ['Red', 1.2, 'Grape'],\n",
    "    ['Red', 1, 'Grape'],\n",
    "    ['Yellow', 3.3, 'Lemon'],\n",
    "    ['Yellow', 3.1, 'Lemon'],\n",
    "    ['Green', 3, 'Apple'],\n",
    "    ['Red', 1.1, 'Grape'],\n",
    "    ['Yellow', 3, 'Lemon'],\n",
    "    ['Red', 1.2, 'Grape'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['color', 'diameter', 'label'], dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#分割數據(訓練集佔80%, 測試集佔20%)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "df_temp = pd.DataFrame()\n",
    "df_temp = df.drop(['diameter'], axis=1)\n",
    "for c in df_temp.columns:\n",
    "    df[c] = LabelEncoder().fit_transform(df[c])\n",
    "\n",
    "y = df['label'].values\n",
    "x = df.drop(['label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 2) (2, 2)\n",
      "(8,) (2,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2, shuffle=True)\n",
    "print(x_train.shape, x_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立隨機森林模型\n",
    "forest_cls = RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=3)\n",
    "\n",
    "#使用隨機森林模型進行訓練\n",
    "forest_cls.fit(x_train, y_train)\n",
    "\n",
    "#以訓練好的隨機森林進行預測\n",
    "y_pred = forest_cls.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
