{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 題目:電商產品評分文件以機器學習方式分辨是否為正向或負向\n",
    "#\n",
    "# 說明：輸入文件 positive.review 和 negative.review，兩者都是XML檔。我們用BeautifulSoup讀進來，\n",
    "# 擷取review_text，然後用NLTK自建Tokenizer。 先產生 word-to-index map 再產生 word-frequency vectors。\n",
    "# 之後 shuffle data 創造 train/test splits，留100個給 test 用。接著用Logistic Regression 分類器\n",
    "# 找出訓練組和測試組的準確度(Accuracy)。接著我們可以看看每個單字的正負權重，可以訂一個閥值，\n",
    "# 比方絕對值大於正負0.5，以確認情緒是顯著的。最後我們找出根據現有演算法歸類錯誤最嚴重的正向情緒和負向\n",
    "# 情緒的例子。\n",
    "#\n",
    "# 延伸:可用不同的tokenizer，不同的tokens_to_vector，不同的ML分類器做改進準確率的比較。最後可用您的\n",
    "# model去預測unlabeled.review檔的內容。\n",
    "#\n",
    "# 範例程式檔名: sentiment_情緒分析.py，以LogisticRegression 方式完成情緒分析。\n",
    "# 模組: sklearn, bs4, numpy, nltk\n",
    "# 輸入檔：stopwords.txt, /electronics 下 positive.review, negative.review\n",
    "# 成績：辨識百分率\n",
    "#\n",
    "#注意事項：nltk 需要有 punkt corpus 和 wordnet  資源\n",
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet') \n",
    "#資料檔需在適當位置 jupyter 或 colab 才能看到，用colab時要上傳 data 到 ./sample_data 或 mount\n",
    "from __future__ import print_function, division\n",
    "from future.utils import iteritems\n",
    "from builtins import range\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import jieba \n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from http://www.lextek.com/manuals/onix/stopwords1.html\n",
    "stopwords = set(w.rstrip() for w in open('stopwords.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 另一個 stopwords 的來源\n",
    "# from nltk.corpus import stopwords\n",
    "# stopwords.words('english')\n",
    "\n",
    "# 讀正向與負向 reviews\n",
    "# data courtesy of http://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html\n",
    "positive_reviews = BeautifulSoup(open('positive.review', encoding='utf-8').read(), features='lxml')\n",
    "positive_reviews = positive_reviews.findAll('review_text')\n",
    "\n",
    "negative_reviews = BeautifulSoup(open('negative.review', encoding='utf-8').read(), features='lxml')\n",
    "negative_reviews = negative_reviews.findAll('review_text')\n",
    "\n",
    "unlabeled_reviews = BeautifulSoup(open('unlabeled.review', encoding='utf-8').read(), features='lxml')\n",
    "unlabeled_reviews = unlabeled_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 基於nltk自建 tokenizer\n",
    "def my_tokenizer(s):\n",
    "    s = s.lower() # downcase\n",
    "    tokens = nltk.tokenize.word_tokenize(s) # 將字串改為tokens\n",
    "    tokens = [t for t in tokens if len(t) > 2] # 去除短字\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # 去除大小寫\n",
    "    tokens = [t for t in tokens if t not in stopwords] # 去除 stopwords\n",
    "    return tokens\n",
    "'''\n",
    "\n",
    "# 使用jieba\n",
    "def my_tokenizer(s):\n",
    "    s = s.lower() # downcase\n",
    "    tokens = jieba.lcut(s) \n",
    "    tokens = [t for t in tokens if len(t) > 2] # 去除短字\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # 去除大小寫\n",
    "    tokens = [t for t in tokens if t not in stopwords] # 去除 stopwords\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先產生 word-to-index map 再產生 word-frequency vectors\n",
    "# 同時儲存 tokenized 版本未來不需再做 tokenization\n",
    "word_index_map = {}\n",
    "current_index = 0\n",
    "positive_tokenized = []\n",
    "negative_tokenized = []\n",
    "unlabeled_tokenized = []\n",
    "orig_reviews = []\n",
    "unlab_reviews = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\User\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.930 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "for review in positive_reviews:\n",
    "    orig_reviews.append(review.text)\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    positive_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review in negative_reviews:\n",
    "    orig_reviews.append(review.text)\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review in unlabeled_reviews:\n",
    "    unlab_reviews.append(review.text)\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    unlabeled_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(word_index_map): 27093\n"
     ]
    }
   ],
   "source": [
    "print(\"len(word_index_map):\", len(word_index_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create our input matrices\n",
    "def tokens_to_vector(tokens, label):\n",
    "    x = np.zeros(len(word_index_map) + 1) # 最後一個元素是標記\n",
    "    y = np.zeros(len(word_index_map) )\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] += 1\n",
    "        y[i] += 1\n",
    "    x = x / x.sum() # 正規化數據提升未來準確度\n",
    "    y = y / y.sum() # 正規化數據提升未來準確度\n",
    "    if label == None:\n",
    "        return y\n",
    "    else:\n",
    "        x[-1] = label\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(positive_tokenized) + len(negative_tokenized)\n",
    "# (N x D+1) 矩陣 - 擺在一塊將來便於shuffle\n",
    "data = np.zeros((N, len(word_index_map) + 1))\n",
    "i = 0\n",
    "test_data = []\n",
    "for tokens in positive_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 1)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "\n",
    "for tokens in negative_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 0)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "\n",
    "for tokens in unlabeled_tokenized:\n",
    "    xy = tokens_to_vector(tokens, None)\n",
    "    test_data.append(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data 創造 train/test splits\n",
    "orig_reviews, data = shuffle(orig_reviews, data)\n",
    "\n",
    "Xtrain = data[:,:-1]\n",
    "Ytrain = data[:,-1]\n",
    "\n",
    "Xtest = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.9065\n",
      "unlabeled review preds: [0. 1. 1. ... 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# 方法1 GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"train score:\", model.score(Xtrain, Ytrain))\n",
    "print(\"unlabeled review preds:\", model.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unlabeled review prob: [0.47144319 0.58174125 0.61032186 ... 0.32116678 0.65921019 0.59519415]\n"
     ]
    }
   ],
   "source": [
    "print(\"unlabeled review prob:\", model.predict_proba(Xtest)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.791\n",
      "unlabeled review preds: [1. 0. 0. ... 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# 方法2 LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"Train accuracy:\", model.score(Xtrain, Ytrain))\n",
    "print(\"unlabeled review preds:\", model.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unlabeled review prob: [0.51052999 0.48280122 0.45929778 ... 0.4768884  0.50207023 0.53503969]\n"
     ]
    }
   ],
   "source": [
    "print(\"unlabeled review prob:\", model.predict_proba(Xtest)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit -0.6839767337739494\n",
      "bad -0.8961762281389208\n",
      "cable 0.7397115156320087\n",
      "time -0.7018078684449433\n",
      "month -0.7922365671726258\n",
      "pro 0.5326834439832305\n",
      "sound 1.1080203119353158\n",
      "lot 0.8222495672164125\n",
      "you 0.9859696446758778\n",
      "easy 1.8718482044932874\n",
      "quality 1.5367618653589483\n",
      "company -0.5736435895717941\n",
      "card -0.5079121381894793\n",
      "item -1.0322462709034472\n",
      "wa -1.6091936531777662\n",
      "perfect 1.0765576791365705\n",
      "fast 1.0122104997265828\n",
      "ha 0.7782993871405728\n",
      "price 2.865441975283546\n",
      "value 0.572901434453409\n",
      "money -1.1270451691206727\n",
      "memory 1.0000927829276396\n",
      "picture 0.6078309662608883\n",
      "buy -0.9437946601016287\n",
      "don -1.0511873118020099\n",
      "bit 0.6334003566308123\n",
      "happy 0.6661380126109168\n",
      "pretty 0.7710884509254797\n",
      "doe -0.7921279822228178\n",
      "highly 1.1002339295548502\n",
      "recommend 0.6789927286956916\n",
      "fit 0.539592200513864\n",
      "customer -0.7031876740645026\n",
      "support -0.9162831181586766\n",
      "little 0.9899799214631945\n",
      "sent -0.5022993129207496\n",
      "returned -0.8421302926291562\n",
      "excellent 1.4338215546015884\n",
      "love 1.2199514652285073\n",
      "video 0.5350287789782135\n",
      "home 0.5594533161964003\n",
      "piece -0.6255236514495274\n",
      "useless -0.5022394751656051\n",
      "week -0.785436452540885\n",
      "size 0.5149377007233417\n",
      "using 0.7203989637528873\n",
      "laptop 0.5592534394682706\n",
      "doesn -0.6203123950411222\n",
      "poor -0.8277681401747813\n",
      "look 0.59047880032136\n",
      "then -1.1437960518524752\n",
      "called -0.5439128813910414\n",
      "tried -0.8606411103533335\n",
      "radio -0.5008493033187319\n",
      "try -0.7164851949622068\n",
      "space 0.6291005094300559\n",
      "comfortable 0.6782512605808124\n",
      "hour -0.5731004430935767\n",
      "expected 0.5890693130925722\n",
      "speaker 0.9499180695625127\n",
      "warranty -0.6410128330885131\n",
      "stopped -0.5501484796666202\n",
      "junk -0.563345943407077\n",
      "returning -0.5428932098576801\n",
      "paper 0.6135874919657279\n",
      "terrible -0.5268919464822487\n",
      "return -1.3499238819319608\n",
      "waste -1.0671834465683303\n",
      "refund -0.6829448221507797\n"
     ]
    }
   ],
   "source": [
    "# 列出每個字的正負 weight\n",
    "# 用不同的 threshold values!\n",
    "threshold = 0.5\n",
    "for word, index in iteritems(word_index_map):\n",
    "    weight = model.coef_[0][index]\n",
    "    if weight > threshold or weight < -threshold:\n",
    "        print(word, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找出歸類錯誤的例子\n",
    "preds = model.predict(Xtrain)\n",
    "P = model.predict_proba(Xtrain)[:,1] # p(y = 1 | x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., ..., 0., 1., 0.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.47173793, 0.51429151, 0.47705149, ..., 0.4901071 , 0.50796664,\n",
       "       0.49108515])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只列出最糟的\n",
    "# Most wrong positive review\n",
    "minP_whenYis1 = 1\n",
    "# Most wrong negative review\n",
    "maxP_whenYis0 = 0\n",
    "wrong_positive_review = None\n",
    "wrong_negative_review = None\n",
    "wrong_positive_prediction = None\n",
    "wrong_negative_prediction = None\n",
    "for i in range(N):\n",
    "    p = P[i]\n",
    "    y = Ytrain[i]\n",
    "    if y == 1 and p < 0.5:\n",
    "        if p < minP_whenYis1:\n",
    "            wrong_positive_review = orig_reviews[i]\n",
    "            wrong_positive_prediction = preds[i]\n",
    "            minP_whenYis1 = p\n",
    "    elif y == 0 and p > 0.5:\n",
    "        if p > maxP_whenYis0:\n",
    "            wrong_negative_review = orig_reviews[i]\n",
    "            wrong_negative_prediction = preds[i]\n",
    "            maxP_whenYis0 = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most wrong positive review (prob = 0.3736172804629852, pred = 0.0):\n",
      "\n",
      "This was a defective unit. Got new unit and it works as expected\n",
      "\n",
      "Most wrong negative review (prob = 0.6027663738328604, pred = 1.0):\n",
      "\n",
      "The Voice recorder meets all my expectations and more\n",
      "Easy to use, easy to transfer great results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Most wrong positive review (prob = %s, pred = %s):\" % (minP_whenYis1, wrong_positive_prediction))\n",
    "print(wrong_positive_review)\n",
    "print(\"Most wrong negative review (prob = %s, pred = %s):\" % (maxP_whenYis0, wrong_negative_prediction))\n",
    "print(wrong_negative_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
