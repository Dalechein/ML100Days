{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dpYyaBgoJLR4"
   },
   "source": [
    "# 自製智能中文選字系統  (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6dC4iuglJLSB",
    "outputId": "8bcc939e-c1a3-46f5-f889-b0fcb97ed34f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.9 (default, Aug 31 2020, 17:10:11) [MSC v.1916 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9fQnVHVmJLSH"
   },
   "source": [
    "## 資料前處理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aQIl0BP5JLSH"
   },
   "source": [
    "確認版本為 python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8wG3IWB2JLSI"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xZXcaXMRJLSL"
   },
   "outputs": [],
   "source": [
    "def prepocess_line(line):\n",
    "    # 僅僅挑出中文字元，並且斷開不連續的中文字\n",
    "    pattern = r'[^\\u4e00-\\u9fa5]+'\n",
    "    segments = re.split(pattern, line)\n",
    "    #segments = [x for x in segments if x!='']\n",
    "    segments = [x for x in segments]\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1V1YPovTJLSO",
    "outputId": "fb9b88ae-9b2e-4ab3-fa8a-9ebec8b5190c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '英語', '一詞源於遷居英格蘭的日耳曼部落盎格魯', '而', '盎格魯', '得名於']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepocess_line('“英語”一詞源於遷居英格蘭的日耳曼部落盎格魯（），而“盎格魯”得名於')  \n",
    "# 應該為：['英語', '一詞源於遷居英格蘭的日耳曼部落盎格魯', '而', '盎格魯', '得名於']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['英語',\n",
       " '英語英語',\n",
       " '又稱爲英文',\n",
       " '是一種西日耳曼語言',\n",
       " '誕生於中世紀早期的英格蘭',\n",
       " '如今具有全球通用語的地位',\n",
       " '英語',\n",
       " '一詞源於遷居英格蘭的日耳曼部落盎格魯',\n",
       " '而',\n",
       " '盎格魯']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments = []\n",
    "with open('./wiki_zh_small.txt', encoding ='utf-8') as fr:\n",
    "    for line in fr.readlines():\n",
    "        segments += prepocess_line(line)\n",
    "segments[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N-E0XU7lJLSQ"
   },
   "source": [
    "## Ngram\n",
    "\n",
    "一開始要先計算字詞出現的次數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U0GNtfVnJLSR"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class Counters:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.counters = [Counter() for _ in range(n + 1)]  # 分別代表計算0、1、...個字的出現次數\n",
    "        #self.counters: [Counter(), Counter(), Counter(), Counter()]\n",
    "\n",
    "    def fit(self, segments):\n",
    "        # 因為 self.counters 分別代表計算0、1、...個字的出現次數\n",
    "        # 請在此實作利用 segments 以及函式 _skip 來統計次數\n",
    "        self.counters[0] = Counter({'': len(''.join(segments))})\n",
    "        for i in range(1,self.n+1):\n",
    "            #for seg in segments:\n",
    "            seg_skip = self._skip(''.join(segments), i)\n",
    "            #print(Counter(seg_skip))\n",
    "            self.counters[i] = Counter(seg_skip)\n",
    "            #self.counters[i] = Counter(seg_skip) 如果 = 0\n",
    "            \n",
    "        return self.counters\n",
    "    \n",
    "    def _skip(self, segment, n):\n",
    "        assert n > 0\n",
    "        if len(segment) < n:\n",
    "            return []\n",
    "        shift = n - 1\n",
    "        for i in range(len(segment) - shift):\n",
    "            yield segment[i:i+shift+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F4-d0YiMJLSU"
   },
   "outputs": [],
   "source": [
    "counters = Counters(n=3)\n",
    "counters = counters.fit(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'': 371370})\n",
      "[('的', 13270), ('國', 4801), ('中', 3944), ('在', 3708), ('一', 3659)]\n",
      "[('中國', 827), ('一個', 674), ('使用', 592), ('年月', 556), ('可以', 510)]\n",
      "[('年月日', 372), ('西班牙', 225), ('聯合國', 212), ('共和國', 212), ('人民共', 188)]\n"
     ]
    }
   ],
   "source": [
    "print(counters[0])\n",
    "print(counters[1].most_common()[:5])\n",
    "print(counters[2].most_common()[:5])\n",
    "print(counters[3].most_common()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立N-Gram模型\n",
    "N-Gram模型在計算詞機率時為(以Trigram為例)\n",
    "$$\n",
    "P(W_i|W_{i-1},W_{i-2}) = \\frac{count(W_i,W_{i-1},W_{i-2})}{count(W_{i-1},W_{i-2})}\n",
    "$$\n",
    "\n",
    "舉例來說\n",
    "$$\n",
    "P(the|this,is) = \\frac{count(this\\ is\\ the)}{count(this\\ is)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ngram:\n",
    "    def __init__(self, n, counters):\n",
    "        #assert n <= counters.n\n",
    "        self.n = n\n",
    "        self.major_counter = counters[n]\n",
    "        #self.minor_counter = counters[n-1]\n",
    "\n",
    "    def predict_proba(self, prefix='', top_k=5):\n",
    "        assert len(prefix) >= self.n - 1\n",
    "        # 使用 Ngram 的公式計算出下一個字出現的機率\n",
    "        # 輸出為機率與字的tuple列表，詳見下方輸出範例\n",
    "        # 應該為：[(0.035732269174118744, '的')\n",
    "        \n",
    "        minor_gram_freq = dict()\n",
    "        probs = dict()\n",
    "        \n",
    "        major_gram_freq = sorted(self.major_counter.items(), key = lambda word_count: word_count[1], reverse=True)\n",
    "        #minor_gram_freq = sorted(self.minor_counter.items(), key = lambda word_count: word_count[1], reverse=True)\n",
    "        #('年月日', 372)\n",
    "        for key in major_gram_freq:\n",
    "            minor_gram_freq[key[0][:2]] = minor_gram_freq.get(key[0][:2],0) + key[1]\n",
    "            \n",
    "        for key2 in major_gram_freq:\n",
    "            if key2[0][:2] not in probs.keys():\n",
    "                probs[key2[0][:2]] = []\n",
    "                probs[key2[0][:2]] = [[key2[1]/minor_gram_freq[key2[0][:2]], key2[0][2]]]\n",
    "            else:\n",
    "                probs[key2[0][:2]] = probs[key2[0][:2]] + [[key2[1]/minor_gram_freq[key2[0][:2]], key2[0][2]]]\n",
    "        \n",
    "        #按第一維排序\n",
    "        probs[prefix].sort(reverse=True)\n",
    "        \n",
    "        #按第二維排序(若有別的案例第二維是數字的話使用)\n",
    "        #probs[prefix].sort(key=lambda x:x[1])\n",
    "        #retrun probs[prefix][:top_k]\n",
    "         \n",
    "        #return sorted_probs[:top_k] if top_k > 0 and len(sorted_probs)>=top_k else sorted_probs   \n",
    "        if len(probs[prefix])>=top_k and top_k>0:\n",
    "            return probs[prefix][:top_k]\n",
    "        else:\n",
    "            return probs[prefix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram = Ngram(3, counters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.75, '故'], [0.25, '維']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram.predict_proba('我思')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.07254901960784314, '用'],\n",
       " [0.06666666666666667, '在'],\n",
       " [0.045098039215686274, '使'],\n",
       " [0.025490196078431372, '是'],\n",
       " [0.023529411764705882, '被']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram.predict_proba('可以')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lG6TV-8iJLSj"
   },
   "source": [
    "## 使用Ngram來建立第一版選字系統"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ak-zlXIGJLSk"
   },
   "outputs": [],
   "source": [
    "class ChineseWordRecommenderV1:\n",
    "    def __init__(self, n=0):\n",
    "        self.n = 0\n",
    "        #self.unigram = unigram\n",
    "        #self.bigram = bigram\n",
    "        #self.trigram = trigram\n",
    "    \n",
    "    def predict_proba(self, prefix='', top_k=5):\n",
    "        minor_gram_freq = dict()\n",
    "        probs = dict()\n",
    "        \n",
    "        #trigram\n",
    "        if len(prefix)>=2:            \n",
    "            prefix = prefix[-2:]\n",
    "            self.n = 3\n",
    "            self.major_counter = counters[self.n]\n",
    "            major_gram_freq = sorted(self.major_counter.items(), key = lambda word_count: word_count[1], reverse=True)\n",
    "            for key in major_gram_freq:\n",
    "                minor_gram_freq[key[0][:2]] = minor_gram_freq.get(key[0][:2],0) + key[1]\n",
    "            for key2 in major_gram_freq:\n",
    "                if key2[0][:2] not in probs.keys():\n",
    "                    probs[key2[0][:2]] = []\n",
    "                    probs[key2[0][:2]] = [[key2[1]/minor_gram_freq[key2[0][:2]], key2[0][2]]]\n",
    "                else:\n",
    "                    probs[key2[0][:2]] = probs[key2[0][:2]] + [[key2[1]/minor_gram_freq[key2[0][:2]], key2[0][2]]]\n",
    "                    \n",
    "            probs[prefix].sort(reverse=True)\n",
    "            \n",
    "            #return sorted_probs[:top_k] if top_k > 0 and len(sorted_probs)>=top_k else sorted_probs \n",
    "            if len(probs[prefix])>=top_k and top_k>0:\n",
    "                return probs[prefix][:top_k]\n",
    "            else:\n",
    "                return probs[prefix]\n",
    "            \n",
    "        #bigram    \n",
    "        elif len(prefix)>=1:\n",
    "            self.n = 2\n",
    "            self.major_counter = counters[self.n]\n",
    "            major_gram_freq = sorted(self.major_counter.items(), key = lambda word_count: word_count[1], reverse=True)\n",
    "            for key in major_gram_freq:\n",
    "                minor_gram_freq[key[0][:1]] = minor_gram_freq.get(key[0][:1],0) + key[1]\n",
    "            for key2 in major_gram_freq:\n",
    "                if key2[0][:1] not in probs.keys():\n",
    "                    probs[key2[0][:1]] = []\n",
    "                    probs[key2[0][:1]] = [[key2[1]/minor_gram_freq[key2[0][:1]], key2[0][1]]]\n",
    "                else:\n",
    "                    probs[key2[0][:1]] = probs[key2[0][:1]] + [[key2[1]/minor_gram_freq[key2[0][:1]], key2[0][1]]]\n",
    "                    \n",
    "            probs[prefix].sort(reverse=True)\n",
    "            \n",
    "            #return sorted_probs[:top_k] if top_k > 0 and len(sorted_probs)>=top_k else sorted_probs \n",
    "            if len(probs[prefix])>=top_k and top_k>0:\n",
    "                return probs[prefix][:top_k]\n",
    "            else:\n",
    "                return probs[prefix] \n",
    "        #unigram    \n",
    "        else:\n",
    "            self.n=1\n",
    "            self.major_counter = counters[self.n]\n",
    "            major_gram_freq = sorted(self.major_counter.items(), key = lambda word_count: word_count[1], reverse=True) \n",
    "            \n",
    "            return major_gram_freq[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0LS4xO4XJLSl"
   },
   "outputs": [],
   "source": [
    "model = ChineseWordRecommenderV1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.8333333333333334, '在'], [0.16666666666666666, '有']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = model.predict_proba('我思故我', top_k=10)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.75, '故'], [0.25, '維']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = model.predict_proba('我思', top_k=10)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.40298507462686567, '們'],\n",
       " [0.07960199004975124, '的'],\n",
       " [0.04477611940298507, '在'],\n",
       " [0.01990049751243781, '是'],\n",
       " [0.01990049751243781, '思'],\n",
       " [0.014925373134328358, '比'],\n",
       " [0.014925373134328358, '不'],\n",
       " [0.009950248756218905, '這'],\n",
       " [0.009950248756218905, '深'],\n",
       " [0.009950248756218905, '最']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = model.predict_proba('我', top_k=10)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 13270),\n",
       " ('國', 4801),\n",
       " ('中', 3944),\n",
       " ('在', 3708),\n",
       " ('一', 3659),\n",
       " ('爲', 3637),\n",
       " ('是', 3600),\n",
       " ('有', 3252),\n",
       " ('人', 3201),\n",
       " ('年', 3188)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = model.predict_proba('', top_k=10)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sM7_WCTmJLSq"
   },
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8zOHOeYLJLSq",
    "outputId": "6ffedaa6-2bbb-49e2-b225-2eb1209da3c1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -U pip\n",
    "#!pip install -q ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "671d1e5f6874463ab4f184d5e8606e81",
      "5e8aac42609740ecb19f192ac9542006"
     ]
    },
    "colab_type": "code",
    "id": "Qsupw9BoJLSs",
    "outputId": "90544490-4fc3-4350-8447-2673999a611f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2c320f25f242b9b5c22aeae6f11c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4191ad8d274c4a0b919493c2ee097877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "text = widgets.Textarea()\n",
    "label = widgets.Label()\n",
    "display(label, text)\n",
    "\n",
    "def func(change):\n",
    "    probs = model.predict_proba(change.new, top_k=10)\n",
    "    label.value = ' ' + '\\t'.join([word for prob, word in probs])\n",
    "\n",
    "text.observe(func, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "final_project_A_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
